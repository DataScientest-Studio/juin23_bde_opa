== Deploying the project

The project has been developed following principles that make for easy iterative development and deployment.

It is split into 3 main sub-parts that run independently from each other, following a philosophy akin to micro-services :

* the financial data reader (which reads data from the external API and saves it into the database)
* the HTTP API (which reads data from the database and makes it available to other components)
* the dashboard (which reads from the API and presents it via a graphical UI)

=== Packaging the project

Packaging is the process by which a piece of software is prepared for distribution and installation in its runtime environment (which is usually distinct from the environment in which it is developed). In Python, it is a well-known headachefootnote:[https://chriswarrick.com/blog/2023/01/15/how-to-improve-python-packaging/#does-python-really-need-virtual-environments], with no established ubiquitous tool nor clear, universal instructions.

Of the numerous options available : pip + venv + setuptools, Hatchfootnote:[https://hatch.pypa.io/], Poetryfootnote:[https://python-poetry.org/], pipenvfootnote:[https://pipenv.pypa.io/], PDMfootnote:[https://pdm.fming.dev/],.. I have elected to go for the latter, which works basically like `npm` does in the Node-JS ecosystem. Simple commands allow to init a skeleton project, add production or development dependencies, build the project for distribution, install it, and so on.

Runtime configuration (e.g. HTTP ports used, database configuration, ...) is handled by Dynaconffootnote:[https://www.dynaconf.com/], which can read configuration either from a file or from environment variables. The secrets required to run the project (API keys, database credentials, ...) are merged into the configuration at startup.

With a properly packaged project, any machine that can run Python can now install and run it. The helper script `run_local.sh` allows to start any of the 3 parts of the project on the local machine, a feature which has proven extremely useful in the development phase (where running a full Docker image is much heavier and slow). An interactive shell (that uses IPython) was even added to allow "playing" with the internal APIs in the same environment as the running application.


=== Orchestrating the project

Though it would be possible to manually launch the 3 different parts of the project in different processes of the same machine, it is not necessarily super practical.

For this reason, both Docker and Docker Compose have been leveraged to allow running the whole project in a consistent way.

The `Dockerfile` has been written with inspiration from the Dockerfile suggested on PDM websitefootnote:[https://pdm.fming.dev/latest/usage/advanced/#use-pdm-in-a-multi-stage-dockerfile], with some modifications to ensure dependencies are not installed on every source code modification.

The `docker-compose.yml` file is quite straightforward, making use of Docker Compose features such as volumesfootnote:[https://docs.docker.com/compose/compose-file/07-volumes/] or secretsfootnote:[https://docs.docker.com/compose/compose-file/09-secrets/]. It defines all the services that make up the application :

* external images : `database` which is simply an official `mongo` imagefootnote:[https://hub.docker.com/_/mongo], properly setup
* internal images : `financial_data_reader`, `data_report`, `internal_api`
* test images : to allow running unit, integration, or functional tests
